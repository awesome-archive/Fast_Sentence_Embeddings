{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fse - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to fse - fast sentence embeddings. The library is intended to compute sentence embeddings as fast as possible. \n",
    "It offers a simple and easy to understand syntax for you to use in your own projects. Before we start with any model, lets have a look at the input types.\n",
    "All fse models require an iterable/generator which produces a tuple. The tuple has two fields: words and index. The index is required for the multi-thread processing, as sentences might not be processed sequentially. The index dictates, which row of the corresponding sentence vector matrix the sentence belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(threadName)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "s = ([\"Hello\", \"world\"], 0)\n",
    "print(s[0])\n",
    "print(s[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words of the tuple will always consist of a list of strings. Otherwise the train method will raise an Error. However, most input data is available as a list of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with this common input format, fse provides the IndexedList and some variants, which handel all required data operations for you. You can provide multiple lists (or sets) which will all be merged into a single list. This eases work if you have to work with the STS datasets.\n",
    "\n",
    "The multiple types of indexed lists. Let's go through them one by one:\n",
    "- IndexedList: for already pre-splitted sentences\n",
    "- **C**IndexedList: for already pre-splitted sentences with a custom index for each sentence\n",
    "- SplitIndexedList: for sentences which have not been splitted. Will split the strings\n",
    "- Split**C**IndexedList: for sentences which have not been splitted and with a custom index for each sentence\n",
    "- **C**SplitIndexedList: for sentences which have not been splitted. Will split the strings. You can provide a custom split function\n",
    "- **C**Split*C*IndexedList: for sentences where you want to provide a custom index and a custom split function.\n",
    "\n",
    "*Note*: These are ordered by speed. Meaning, that IndexedList is the fastest, while **C**Split*C*IndexedList is the slowest variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 12:06:10,972 : MainThread : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Hello', 'there'], 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fse import SplitIndexedList\n",
    "\n",
    "sentences_a = [\"Hello there\", \"how are you?\"]\n",
    "sentences_b = [\"today is a good day\", \"Lorem ipsum\"]\n",
    "\n",
    "s = SplitIndexedList(sentences_a, sentences_b)\n",
    "print(len(s))\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save memory, we do not convert the original lists inplace. The conversion will only take place once you call the getitem method. To access the original data, call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello there', 'how are you?', 'today is a good day', 'Lorem ipsum']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is already preprocessed as a list of lists you can just use the IndexedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Hello', 'there'], 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fse import IndexedList\n",
    "\n",
    "sentences_splitted = [\"Hello there\".split(), \"how are you?\".split()]\n",
    "s = IndexedList(sentences_splitted)\n",
    "print(len(s))\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to provide your own splitting function, you can pass a callable to the **C**SplitIndexedList class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['hello', 'there'], 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fse import CSplitIndexedList\n",
    "\n",
    "def split_func(string):\n",
    "    return string.lower().split()\n",
    "\n",
    "s = CSplitIndexedList(sentences_a, custom_split=split_func)\n",
    "print(len(s))\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to stream a file from disk (where each line corresponds to a sentence) you can use the IndexedLineDocument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fse import IndexedLineDocument\n",
    "doc = IndexedLineDocument(\"../fse/test/test_data/test_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t['Good', 'stuff', 'i', 'just', 'wish', 'it', 'lasted', 'longer']\n",
      "1\t['Hp', 'makes', 'qualilty', 'stuff']\n",
      "2\t['I', 'like', 'it']\n",
      "3\t['Try', 'it', 'you', 'will', 'like', 'it']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for s in doc:\n",
    "    print(f\"{s[1]}\\t{s[0]}\")\n",
    "    i += 1\n",
    "    if i == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are later working with the similarity of sentences, the IndexedLineDocument provides you the option to access each line by its corresponding index. This helps you in determining the similarity of sentences, as the most_similar method would otherwise just return indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I feel like i just got screwed'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model / Performing inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a fse model is simple. You only need a pre-trained word embedding model which you use during the initializiation of the fse model you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 12:06:12,050 : MainThread : INFO : loading projection weights from /Users/oliverborchers/gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.gz\n",
      "2019-09-11 12:06:56,394 : MainThread : INFO : loaded (400000, 100) matrix from /Users/oliverborchers/gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.gz\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "data = api.load(\"quora-duplicate-questions\")\n",
    "glove = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverborchers/anaconda3/envs/fsedev/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6468640\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for d in data:\n",
    "    # Let's blow up the data a bit by replicating each sentence.\n",
    "    for i in range(8):\n",
    "        sentences.append(d[\"question1\"].split())\n",
    "        sentences.append(d[\"question2\"].split())\n",
    "s = IndexedList(sentences)\n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have about 6468640 sentences we want to compute the embeddings for. If you import the FAST_VERSION variable as follows you can ensure, that the compiliation of the cython routines worked correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from fse.models.average import FAST_VERSION, MAX_WORDS_IN_BATCH\n",
    "print(MAX_WORDS_IN_BATCH)\n",
    "print(FAST_VERSION)\n",
    "# 1 -> The fast version works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fse.models import SIF\n",
    "model = SIF(glove, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 12:07:19,386 : MainThread : INFO : scanning all indexed sentences and their word counts\n",
      "2019-09-11 12:07:24,388 : MainThread : INFO : SCANNING : finished 3902109 sentences with 43127298 words\n",
      "2019-09-11 12:07:27,459 : MainThread : WARNING : found 16 empty sentences\n",
      "2019-09-11 12:07:27,459 : MainThread : INFO : finished scanning 6468640 sentences with an average length of 11 and 71556728 total words\n",
      "2019-09-11 12:07:27,588 : MainThread : INFO : estimated memory for 6468640 sentences with 100 dimensions and 400000 vocabulary: 2621 MB (2 GB)\n",
      "2019-09-11 12:07:27,589 : MainThread : INFO : initializing sentence vectors for 6468640 sentences\n",
      "2019-09-11 12:07:44,587 : MainThread : INFO : pre-computing SIF weights for 400000 words\n",
      "2019-09-11 12:07:44,895 : MainThread : INFO : begin training\n",
      "2019-09-11 12:07:49,906 : MainThread : INFO : PROGRESS : finished 32.32% with 2090654 sentences and 15883331 words, 418130 sentences/s\n",
      "2019-09-11 12:07:54,906 : MainThread : INFO : PROGRESS : finished 65.37% with 4228683 sentences and 32184398 words, 427605 sentences/s\n",
      "2019-09-11 12:07:59,952 : MainThread : INFO : PROGRESS : finished 96.83% with 6263314 sentences and 47676135 words, 406926 sentences/s\n",
      "2019-09-11 12:08:00,998 : MainThread : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-09-11 12:08:00,999 : MainThread : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-09-11 12:08:23,293 : MainThread : INFO : computing 1 principal components took 22s\n",
      "2019-09-11 12:08:25,847 : MainThread : INFO : removing 1 principal components took 2s\n",
      "2019-09-11 12:08:25,848 : MainThread : INFO : training on 6468624 effective sentences with 49255184 effective words took 16s with 401666 sentences/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6468624, 49255184)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models training speed revolves at around 400,000 - 500,000 sentences / seconds. That means we finish the task in about 10 seconds. This is **heavily dependent** on the input processing. If you train ram-to-ram it is naturally faster than any ram-to-disk or disk-to-disk varianty. Similarly, the speed depends on the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the sif model is trained, you can perform additional inferences for unknown sentences. This two step process for new data is required, as computing the principal components for models like SIF and uSIF will require a fair amount of sentences. If you want the vector for a single sentence (which is out of the training vocab), just use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 12:08:25,866 : MainThread : INFO : scanning all indexed sentences and their word counts\n",
      "2019-09-11 12:08:25,868 : MainThread : INFO : finished scanning 1 sentences with an average length of 3 and 3 total words\n",
      "2019-09-11 12:08:25,874 : MainThread : INFO : removing 1 principal components took 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.381964  , -0.19883852,  0.05714178, -0.26080537, -0.39127582,\n",
       "         0.56997186, -0.08829714,  0.04303323,  0.02415925, -0.18261646,\n",
       "         0.12377759,  0.16709256,  0.22155528,  0.19370778, -0.263913  ,\n",
       "        -0.28011692, -0.26631504, -0.04989926, -0.28030562,  0.35429248,\n",
       "        -0.33491367,  0.4502715 , -0.3593142 , -0.15993649,  0.19240798,\n",
       "         0.50069076, -0.45003757, -0.25315157,  0.7854811 ,  0.1651807 ,\n",
       "         0.4882131 ,  0.38489464,  0.71546996,  0.14502618, -0.2963435 ,\n",
       "         0.2826286 , -0.11362529,  0.04488423,  0.41105896, -0.24948171,\n",
       "         0.16784175,  0.18146743,  0.56374484, -0.16483444, -0.34314325,\n",
       "        -0.11086494, -0.71529436,  0.37857842,  0.549762  , -0.21101838,\n",
       "        -0.15454581, -0.07437952, -0.0044148 , -0.0836513 ,  0.20791832,\n",
       "         0.01463795,  0.27646336,  0.39419648, -0.30494606, -0.09648022,\n",
       "         0.47651437,  0.87657607, -0.01992241, -0.23987049, -0.14416814,\n",
       "         0.04672481,  0.27598107,  0.09775755,  0.10615338,  0.03958496,\n",
       "        -0.06071427,  0.6741647 ,  0.49580413, -0.67449296, -0.24942371,\n",
       "         0.75952137,  0.29493946, -0.3692709 ,  0.27619424,  0.11775666,\n",
       "        -0.32955045,  0.62929296,  0.6492067 , -0.3535994 , -0.42289495,\n",
       "        -0.14429063, -0.54398006,  0.11261498, -0.51643586, -0.00974181,\n",
       "        -0.2068301 ,  0.27616104,  1.1002473 ,  0.11245029, -0.2949998 ,\n",
       "        -0.32002652, -0.23377608,  0.27291483, -0.30762812,  0.54437554]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = (\"Hello my friends\".split(), 0)\n",
    "model.infer([tmp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to query the model or perform similarity computations we can just access the model.sv (sentence vectors) object and use its method. To get a vector for an index, just call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10094467, -0.17338434, -0.06120607,  0.34027657,  0.22177696,\n",
       "       -0.24794774, -0.11074719, -0.14664021, -0.05048759, -0.00544697,\n",
       "       -0.1003525 , -0.00855931, -0.12817517, -0.23004776, -0.1799999 ,\n",
       "        0.07368518, -0.03067313, -0.05431814,  0.08335009, -0.10299531,\n",
       "        0.068488  , -0.00242307, -0.04005107,  0.29239285, -0.08163965,\n",
       "       -0.0883536 ,  0.1218466 ,  0.1400187 , -0.07441948,  0.14240132,\n",
       "       -0.15721212, -0.02932566, -0.06760302, -0.07329973, -0.03554938,\n",
       "        0.08278871,  0.19171   , -0.05927336, -0.14163393, -0.1375518 ,\n",
       "       -0.02026191, -0.04314214, -0.05729384,  0.0653533 , -0.06144975,\n",
       "        0.06533504, -0.09953416,  0.04081041, -0.03480543, -0.19186741,\n",
       "       -0.21262895,  0.09202442,  0.10283872,  0.12685192, -0.16222198,\n",
       "       -0.21388721, -0.24895632, -0.16497527,  0.29118574, -0.19552866,\n",
       "       -0.0310809 , -0.3014589 , -0.00456905,  0.11545968, -0.11587107,\n",
       "        0.06897421, -0.25824726,  0.0791491 ,  0.16394164,  0.06048156,\n",
       "        0.02214444, -0.01330999, -0.3030917 , -0.18095425, -0.13198662,\n",
       "       -0.12820065, -0.09165528,  0.04982938, -0.02272266, -0.08077918,\n",
       "        0.04727352,  0.23615004,  0.03795493,  0.04745507,  0.3396399 ,\n",
       "       -0.01102063, -0.02038458, -0.10761918, -0.00284445, -0.0022642 ,\n",
       "        0.07875209,  0.02067171, -0.09319284, -0.02075411, -0.06282291,\n",
       "        0.17833623,  0.21346036, -0.04972732,  0.01862234, -0.04622836],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the similarity or distance between two sentence from the training set you can call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929\n",
      "0.071\n"
     ]
    }
   ],
   "source": [
    "print(model.sv.similarity(0,1).round(3))\n",
    "print(model.sv.distance(0,1).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further call for the most similar sentences given an index. For example, we want to know the most similar sentences for sentence index 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Should', 'I', 'buy', 'tiago?'], 100)\n"
     ]
    }
   ],
   "source": [
    "print(s[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 12:08:25,930 : MainThread : INFO : precomputing L2-norms of sentence vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3949083, 0.9999999403953552),\n",
       " (897678, 0.9999999403953552),\n",
       " (4229890, 0.9999999403953552),\n",
       " (3949079, 0.9999999403953552),\n",
       " (3949081, 0.9999999403953552),\n",
       " (2934317, 0.9999999403953552),\n",
       " (4093542, 0.9999999403953552),\n",
       " (3949075, 0.9999999403953552),\n",
       " (4229889, 0.9999999403953552),\n",
       " (2934319, 0.9999999403953552)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sv.most_similar(100)\n",
    "# Division by zero can happen if you encounter empy sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the preceding function will only supply the indices of the most similar sentences. You can circumvent this problem by passing an indexable function to the most_similar call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Should', 'I', 'buy', 'Asus', 'Zenfone', '5?'],\n",
       "  3949083,\n",
       "  0.9999999403953552),\n",
       " (['Why', \"doesn't\", 'Google', 'buy', 'Quora?'], 897678, 0.9999999403953552),\n",
       " (['Should', 'Google', 'buy', 'Quora?'], 4229890, 0.9999999403953552),\n",
       " (['Should', 'I', 'buy', 'Asus', 'Zenfone', '5?'],\n",
       "  3949079,\n",
       "  0.9999999403953552),\n",
       " (['Should', 'I', 'buy', 'Asus', 'Zenfone', '5?'],\n",
       "  3949081,\n",
       "  0.9999999403953552),\n",
       " (['Why', \"didn't\", 'Facebook', 'buy', 'Twitter?'],\n",
       "  2934317,\n",
       "  0.9999999403953552),\n",
       " (['Should', 'I', 'buy', 'Xiaomi', 'Redmi', 'Note', '3?', 'Why?'],\n",
       "  4093542,\n",
       "  0.9999999403953552),\n",
       " (['Should', 'I', 'buy', 'Asus', 'Zenfone', '5?'],\n",
       "  3949075,\n",
       "  0.9999999403953552),\n",
       " (['Will', 'Google', 'buy', 'Quora?'], 4229889, 0.9999999403953552),\n",
       " (['Why', \"didn't\", 'Facebook', 'buy', 'Twitter?'],\n",
       "  2934319,\n",
       "  0.9999999403953552)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sv.most_similar(100, indexable=s.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go. This is a lot more understandable than the initial list of indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search for sentences, which are similar to a given word vector, you can call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['How', 'do', 'I', 'make', 'easy', 'money?'], 6383261, 0.5409939289093018),\n",
       " (['How', 'do', 'I', 'make', 'easy', 'money?'], 842112, 0.5409939289093018),\n",
       " (['How', 'do', 'I', 'make', 'easy', 'money?'], 6383255, 0.5409939289093018),\n",
       " (['How', 'do', 'I', 'make', 'easy', 'money?'], 6383257, 0.5409939289093018),\n",
       " (['How', 'do', 'I', 'make', 'easy', 'money?'], 6383259, 0.5409939289093018),\n",
       " (['How', 'do', 'I', 'make', 'easy', 'money?'], 4405391, 0.5409939289093018),\n",
       " (['How', 'do', 'I', 'make', 'easy', 'money?'], 6383263, 0.5409939289093018),\n",
       " (['How', 'do', 'I', 'make', 'easy', 'money?'], 842116, 0.5409939289093018),\n",
       " (['How', 'do', 'I', 'make', 'easy', 'money?'], 842114, 0.5409939289093018),\n",
       " (['How', 'do', 'I', 'make', 'easy', 'money?'], 6383251, 0.5409939289093018)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sv.similar_by_word(\"easy\", wv=glove, indexable=s.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, you can query for unknown (or new) sentences by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-09-11 12:08:35,263 : MainThread : INFO : scanning all indexed sentences and their word counts\n",
      "2019-09-11 12:08:35,266 : MainThread : INFO : finished scanning 1 sentences with an average length of 6 and 6 total words\n",
      "2019-09-11 12:08:35,270 : MainThread : INFO : removing 1 principal components took 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Arabic?'],\n",
       "  4666689,\n",
       "  0.9029382467269897),\n",
       " (['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Arabic',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'first?'],\n",
       "  4666700,\n",
       "  0.9029382467269897),\n",
       " (['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Arabic',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'first?'],\n",
       "  4666698,\n",
       "  0.9029382467269897),\n",
       " (['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Arabic',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'first?'],\n",
       "  4666690,\n",
       "  0.9029382467269897),\n",
       " (['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Arabic?'],\n",
       "  4666691,\n",
       "  0.9029382467269897),\n",
       " (['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Arabic?'],\n",
       "  4666699,\n",
       "  0.9029382467269897),\n",
       " (['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Arabic',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'first?'],\n",
       "  4666692,\n",
       "  0.9029382467269897),\n",
       " (['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Arabic?'],\n",
       "  4666693,\n",
       "  0.9029382467269897),\n",
       " (['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Arabic',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'first?'],\n",
       "  4666694,\n",
       "  0.9029382467269897),\n",
       " (['Is',\n",
       "   'it',\n",
       "   'easy',\n",
       "   'to',\n",
       "   'learn',\n",
       "   'Hebrew',\n",
       "   'if',\n",
       "   'you',\n",
       "   'learn',\n",
       "   'Arabic?'],\n",
       "  4666695,\n",
       "  0.9029382467269897)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sv.similar_by_sentence(\"Is this really easy to learn\".split(), model=model, indexable=s.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to browse through the library and get to know the functions a little better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
